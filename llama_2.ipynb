{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d88da10-8ed0-4eb0-b5e7-084272feda4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers==4.31.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: datasets==2.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: peft==0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: accelerate==0.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: bitsandbytes==0.40.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.40.2)\n",
      "Requirement already satisfied: trl==0.4.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.4.7)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.3.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31.0) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31.0) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31.0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31.0) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31.0) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31.0) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31.0) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.31.0) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.13.0) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.13.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.13.0) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.13.0) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.13.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.13.0) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.13.0) (3.8.5)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft==0.4.0) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft==0.4.0) (2.0.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.31.0) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.31.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.31.0) (2023.5.7)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets==2.13.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets==2.13.0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.13.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83898e91-d743-4cd9-9a8c-5b27374de012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e8e532f-6781-462e-a220-ba532eb74879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"iamtarun/python_code_instructions_18k_alpaca\"\n",
    "#dataset_name = \"HuggingFaceH4/CodeAlpaca_20K\"\n",
    "# Dataset split\n",
    "dataset_split= \"train\"\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-int4-python-code-20k\"\n",
    "# Huggingface repository\n",
    "hf_model_repo=\"zeeshanali00/\"+new_model\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_double_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = new_model\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1 # 2\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-3 #1e-5\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\" #\"constant\"\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = False\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "# Disable tqdm\n",
    "disable_tqdm= True\n",
    "\n",
    "################################################################################\n",
    "# SFTTrainer parameters\n",
    "################################################################################\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 2048 #None\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = True #False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1538a5e-3612-462c-ae95-c7cbc1a08675",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "import os\n",
    "\n",
    "# Load the enviroment variables\n",
    "\n",
    "# Login to the Hugging Face Hub\n",
    "login(token='**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73c51422-45d2-4ec1-a05a-1f40dc479868",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ec2-user/.cache/huggingface/datasets/iamtarun___parquet/iamtarun--python_code_instructions_18k_alpaca-cfc26604e43ea064/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 18612\n",
      "{'instruction': 'Create a web-crawler in Python to extract links from a web page.', 'input': 'Web page URL: http://www.example.com', 'output': 'import urllib.request \\nfrom bs4 import BeautifulSoup\\n\\n#Providing the url as an argument to urlopen() \\nurl = \"http://www.example.com\"\\nhtml = urllib.request.urlopen(url).read() \\nsoup = BeautifulSoup(html, features = \\'html.parser\\') \\n\\n# extracting links from the web page \\nlinks = [] \\nfor link in soup.findAll(\\'a\\'): \\n    links.append(link.get(\\'href\\')) \\n\\nprint(links)', 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate a web-crawler in Python to extract links from a web page.\\n\\n### Input:\\nWeb page URL: http://www.example.com\\n\\n### Output:\\nimport urllib.request \\nfrom bs4 import BeautifulSoup\\n\\n#Providing the url as an argument to urlopen() \\nurl = \"http://www.example.com\"\\nhtml = urllib.request.urlopen(url).read() \\nsoup = BeautifulSoup(html, features = \\'html.parser\\') \\n\\n# extracting links from the web page \\nlinks = [] \\nfor link in soup.findAll(\\'a\\'): \\n    links.append(link.get(\\'href\\')) \\n\\nprint(links)'}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from the hub\n",
    "dataset = load_dataset(dataset_name, split=dataset_split)\n",
    "# Show dataset size\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "# Show an example\n",
    "print(dataset[randrange(len(dataset))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0313ad7f-b9c8-4a22-97be-f3080a715c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the instruction format for iamtarun/python_code_instructions_18k_alpaca\n",
    "def format_instruction(sample):\n",
    "\treturn f\"\"\"### Instruction:\n",
    "Use the Task below and the Input given to write the Response, which is a programming code that can solve the following Task:\n",
    "\n",
    "### Task:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Input:\n",
    "{sample['input']}\n",
    "\n",
    "### Response:\n",
    "{sample['output']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76fa00e6-498b-42ce-9e56-593aab2ad5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the type\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_use_double_quant=use_double_nested_quant,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b50f7ec0-b6f5-4260-96b7-cdd60d7350cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2667ebd0940245f2a924ba23e3b7c51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, use_cache = False, device_map=device_map)\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa21bbe0-b5f6-421b-abb2-f2382bbf4822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        r=lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# Not necessary when using SFTTrainer\n",
    "# prepare model for training\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82fc96ac-ff49-4bdc-ae4d-889c820276a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size, # 6 if use_flash_attention else 4,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    optim=optim,\n",
    "    #save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    max_steps=2,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    disable_tqdm=disable_tqdm,\n",
    "    report_to=\"tensorboard\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caa79da4-5fd8-4e36-86fd-bb9d83e5d6ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.14.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (1.57.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (3.4.4)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (2.29.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (67.7.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (2.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (0.40.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.14)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3d1c64b-c40c-43cb-ab8c-f89ef0da3d75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=packing,\n",
    "    formatting_func=format_instruction,\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b45c1538-75fa-4fe5-9820-ed594512994d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 55.205, 'train_samples_per_second': 0.145, 'train_steps_per_second': 0.036, 'train_loss': 1.1212422847747803, 'epoch': 0.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train\n",
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    "\n",
    "# save model in local\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc181fc-85e5-4d1e-aeb7-38c3e72cb958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Empty VRAM\n",
    "#del model\n",
    "#del trainer\n",
    "#import gc\n",
    "#gc.collect()\n",
    "#gc.collect()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "999278a3-f2e9-4d38-91a3-0f3a85598c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d9266c1-d847-4d3b-a59c-d1246ac46ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60001252b46f4e2388d8c07b6f1c3fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('merged_model/tokenizer_config.json',\n",
       " 'merged_model/special_tokens_map.json',\n",
       " 'merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "device_map = {\"\": 0}\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    " args.output_dir,\n",
    " low_cpu_mem_usage=True,\n",
    " return_dict=True,\n",
    " torch_dtype=torch.float16,\n",
    " device_map=device_map,\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51ccf5-ddc2-4297-8cae-e54b4a82728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the enviroment variables\n",
    "load_dotenv()\n",
    "# Login to the Hugging Face Hub\n",
    "login(token='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f18fd4a8-1618-4a82-8757-1f76b181405e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/zeeshanali00/llama-2-7b-int4-python-code-20k/commit/2da1b1921a1e9a3c0c53cb0ab36e68aec8300f90', commit_message='Upload tokenizer', commit_description='', oid='2da1b1921a1e9a3c0c53cb0ab36e68aec8300f90', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.push_to_hub(hf_model_repo,token='**')\n",
    "tokenizer.push_to_hub(hf_model_repo,token='**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "484a9bd6-d152-47a7-b403-522c7e0c809d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeb1e921-3438-4a4f-b945-89b2399e9dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': \"Create a class in Python that inherits properties from a parent class named 'Animal'.\",\n",
       " 'input': '',\n",
       " 'output': 'class Animal:\\n    def __init__(self, name):\\n        self.name = name\\n\\nclass Dog(Animal):\\n    def __init__(self, name, breed):\\n        super().__init__(name)\\n        self.breed = breed',\n",
       " 'prompt': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate a class in Python that inherits properties from a parent class named 'Animal'.\\n\\n### Input:\\n\\n\\n### Output:\\nclass Animal:\\n    def __init__(self, name):\\n        self.name = name\\n\\nclass Dog(Animal):\\n    def __init__(self, name, breed):\\n        super().__init__(name)\\n        self.breed = breed\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a436ca0-e01e-44e3-8625-c21edea035e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!kill -9 23456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fb37cce-d29f-4a5c-808f-01b1f7af20b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af4dda16-9031-4a0c-8f32-01ba347c6bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5994b4b97a8045549d8d78cd86fe1f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   835,  2799,  4080, 29901,    13, 11403,   278,  9330,  2400,\n",
      "           322,   278, 10567,  2183,   304,  2436,   278, 13291, 29892,   607,\n",
      "           338,   263,  8720,   775,   393,   508,  4505,   278,  9330, 29889,\n",
      "            13,    13,  2277, 29937,  9330, 29901,    13, 20624,   326,   675,\n",
      "           263,   775, 11534,  3971,   297,  5132, 29889,   450,   775, 11534,\n",
      "           881,  1653,   263,  1051,   310,  3694,   515, 29871, 29900,   304,\n",
      "         29871, 29896, 29900,   393,   526,  8572,  1821,   491, 29871, 29906,\n",
      "         29889,    13,    13,  2277, 29937, 10567, 29901,    13,    13,    13,\n",
      "          2277, 29937, 13291, 29901,    13,    13,    13,  2277, 29937, 10604,\n",
      "         29901,    13,    13,    13,  2277, 29937,  9330, 29901,    13, 20624,\n",
      "           326,   675,   263,   775, 11534,  3971,   297,  5132, 29889,   450,\n",
      "           775, 11534,   881,  1653,   263,  1051,   310,  3694,   515, 29871,\n",
      "         29900,   304, 29871, 29896, 29900,   393,   526,  8572,  1821,   491,\n",
      "         29871, 29906, 29889,    13,    13,  2277, 29937, 10567, 29901,    13,\n",
      "            13,    13,  2277, 29937, 13291, 29901,    13,    13,    13,  2277,\n",
      "         29937, 10604, 29901,    13,    13,    13,  2277, 29937,  9330, 29901,\n",
      "            13, 20624,   326,   675,   263,   775, 11534,  3971,   297,  5132,\n",
      "         29889,   450,   775, 11534,   881,  1653,   263,  1051,   310,  3694,\n",
      "           515, 29871, 29900,   304, 29871]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device_map = {\"\": 0}\n",
    "# Get the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('zeeshanali00/llama-2-7b-int4-python-code-20k')\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained('zeeshanali00/llama-2-7b-int4-python-code-20k', load_in_4bit=True,\n",
    " torch_dtype=torch.float16,\n",
    " device_map=device_map)\n",
    "# Create an instruction\n",
    "instruction=\"Optimize a code snippet written in Python. The code snippet should create a list of numbers from 0 to 10 that are divisible by 2.\"\n",
    "input=\"\"\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Task below and the Input given to write the Response, which is a programming code that can solve the Task.\n",
    "\n",
    "### Task:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# Run the model to infere an output\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.5)\n",
    "\n",
    "# Print the result\n",
    "#print(f\"Prompt:\\n{prompt}\\n\")\n",
    "#print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0880415f-013a-475d-9045-1d4175213eea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af78fdead6f48ad8b9fda6a6952da51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "### Instruction:\n",
      "Use the Task below and the Input given to write the Response, which is a programming code that can solve the Task.\n",
      "\n",
      "### Task:\n",
      "Write a Python function to display the first and last elements of a list.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "Generated instruction:\n",
      "\n",
      "```python\n",
      "def my_function(list_to_process):\n",
      "    result_list = []\n",
      "    first_element = ''\n",
      "    last_element = ''\n",
      "\n",
      "    for element in list_to_process:\n",
      "        result_list.append(element)\n",
      "        if element == list_to_process[-1]:\n",
      "            last_element = element\n",
      "\n",
      "    return f'{first_element} {last_element}'\n",
      "```\n",
      "\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device_map = {\"\": 0}\n",
    "# Get the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('zeeshanali00/llama-2-7b-int4-python-code-20k')\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained('zeeshanali00/llama-2-7b-int4-python-code-20k', load_in_4bit=True, torch_dtype=torch.float16,\n",
    "                                             device_map=device_map)\n",
    "\n",
    "# Create an instruction\n",
    "instruction=\"Write a Python function to display the first and last elements of a list.\"\n",
    "input=\"\"\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Task below and the Input given to write the Response, which is a programming code that can solve the Task.\n",
    "\n",
    "### Task:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# Run the model to infere an output\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
